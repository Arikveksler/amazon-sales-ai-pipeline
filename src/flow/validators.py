"""
Validators Module
=================
×¤×•× ×§×¦×™×•×ª validation ×œ×‘×“×™×§×ª ×›×œ ×©×œ×‘×™ ×”-Pipeline.

×›×œ ×¤×•× ×§×¦×™×” ××—×–×™×¨×” (bool, str):
- (True, ×”×•×“×¢×”) - ×× ×”validation ×¢×‘×¨
- (False, ×”×•×“×¢×”) - ×× ×”validation × ×›×©×œ

Author: Pipeline Lead
Date: 2026
"""

from pathlib import Path
import json
import pandas as pd
from loguru import logger
from typing import Tuple


def validate_raw_data(file_path: str) -> Tuple[bool, str]:
    """
    ×‘×“×™×§×ª ×ª×§×™× ×•×ª × ×ª×•× ×™× ×’×•×œ××™×™×.
    ××•×•×“× ×©×”×§×•×‘×¥ ×§×™×™×, × ×™×ª×Ÿ ×œ×˜×¢×™× ×”, ×•××›×™×œ × ×ª×•× ×™×.

    Args:
        file_path: × ×ª×™×‘ ×œ×§×•×‘×¥ ×”× ×ª×•× ×™× ×”×’×•×œ××™

    Returns:
        (True, ×”×•×“×¢×”) ×× ×”×§×•×‘×¥ ×ª×§×™×Ÿ
        (False, ×”×•×“×¢×”) ×× ×™×© ×‘×¢×™×”
    """
    logger.info(f"ğŸ” Validating raw data: {file_path}")

    try:
        # ×”××¨×” ×œ-Path object
        file_path_obj = Path(file_path)

        # ×‘×“×™×§×ª ×§×™×•× ×”×§×•×‘×¥
        if not file_path_obj.exists():
            msg = f"File not found: {file_path}"
            logger.error(f"âœ— {msg}")
            return False, msg

        # ×˜×¢×™× ×ª ×”× ×ª×•× ×™×
        df = pd.read_csv(file_path)

        # ×‘×“×™×§×ª ×©×•×¨×•×ª - ×”×× ×™×© ×œ×¤×—×•×ª ×©×•×¨×” ××—×ª
        if len(df) == 0:
            msg = "File is empty (no rows)"
            logger.error(f"âœ— {msg}")
            return False, msg

        # ×‘×“×™×§×ª ×¢××•×“×•×ª - ×”×× ×™×© ×œ×¤×—×•×ª ×¢××•×“×” ××—×ª
        if len(df.columns) == 0:
            msg = "File has no columns"
            logger.error(f"âœ— {msg}")
            return False, msg

        # ×”×¦×œ×—×” - ×”×›×œ ×ª×§×™×Ÿ
        msg = f"Raw data validation passed: {len(df)} rows, {len(df.columns)} columns"
        logger.success(f"âœ“ {msg}")
        return True, msg

    except pd.errors.EmptyDataError as e:
        msg = f"File is empty or corrupted: {str(e)}"
        logger.error(f"âœ— {msg}")
        return False, msg

    except pd.errors.ParserError as e:
        msg = f"Failed to parse CSV file: {str(e)}"
        logger.error(f"âœ— {msg}")
        return False, msg

    except Exception as e:
        msg = f"Error validating raw data: {str(e)}"
        logger.error(f"âœ— {msg}")
        return False, msg


def validate_clean_data(file_path: str, contract_path: str) -> Tuple[bool, str]:
    """
    ×‘×“×™×§×ª ×ª×§×™× ×•×ª × ×ª×•× ×™× ×× ×•×§×™× ××•×œ ×”×—×•×–×”.
    ××•×•×“× ×©×›×œ ×”×¢××•×“×•×ª ×”× ×“×¨×©×•×ª ×§×™×™××•×ª ×•××™×Ÿ ×‘×”×Ÿ ×¢×¨×›×™× ×—×¡×¨×™×.

    Args:
        file_path: × ×ª×™×‘ ×œ×§×•×‘×¥ ×”× ×ª×•× ×™× ×”×× ×•×§×™× (clean_data.csv)
        contract_path: × ×ª×™×‘ ×œ×§×•×‘×¥ ×”×—×•×–×” (dataset_contract.json)

    Returns:
        (True, ×”×•×“×¢×”) ×× ×”× ×ª×•× ×™× ×ª×•×××™× ×œ×—×•×–×”
        (False, ×”×•×“×¢×”) ×× ×™×© ××™-×”×ª×××”
    """
    logger.info(f"ğŸ” Validating clean data against contract")
    logger.info(f"   Data file: {file_path}")
    logger.info(f"   Contract file: {contract_path}")

    try:
        # ×‘×“×™×§×ª ×§×™×•× ×§×•×‘×¥ ×”× ×ª×•× ×™× ×”×× ×•×§×™×
        file_path_obj = Path(file_path)
        if not file_path_obj.exists():
            msg = f"Clean data file not found: {file_path}"
            logger.error(f"âœ— {msg}")
            return False, msg

        # ×‘×“×™×§×ª ×§×™×•× ×§×•×‘×¥ ×”×—×•×–×”
        contract_path_obj = Path(contract_path)
        if not contract_path_obj.exists():
            msg = f"Contract file not found: {contract_path}"
            logger.error(f"âœ— {msg}")
            return False, msg

        # ×˜×¢×™× ×ª ×”-DataFrame
        logger.info("   Loading clean data...")
        df = pd.read_csv(file_path)

        # ×˜×¢×™× ×ª ×”×—×•×–×”
        logger.info("   Loading contract...")
        with open(contract_path, "r", encoding="utf-8") as f:
            contract = json.load(f)

        # ×§×‘×œ×ª ×”×¢××•×“×•×ª ×”× ×“×¨×©×•×ª ××”×—×•×–×”
        # ×‘×“×™×§×” ×× ×™×© required_columns, ××—×¨×ª × ×©×ª××© ×‘×¢××•×“×•×ª ××”-schema
        if "required_columns" in contract:
            required_columns = contract["required_columns"]
        elif "schema" in contract and "columns" in contract["schema"]:
            required_columns = contract["schema"]["columns"]
        else:
            # ×× ××™×Ÿ ×”×’×“×¨×” ×¡×¤×¦×™×¤×™×ª, × × ×™×— ×©×›×œ ×”×¢××•×“×•×ª × ×“×¨×©×•×ª
            msg = "Contract missing required_columns definition, skipping column check"
            logger.warning(f"âš  {msg}")
            required_columns = []

        # ×‘×“×™×§×ª ×¢××•×“×•×ª ×—×¡×¨×•×ª
        if required_columns:
            missing_cols = set(required_columns) - set(df.columns)
            if missing_cols:
                msg = f"Missing required columns: {missing_cols}"
                logger.error(f"âœ— {msg}")
                return False, msg
            logger.info(f"   âœ“ All {len(required_columns)} required columns present")

            # ×‘×“×™×§×ª missing values ×‘×¢××•×“×•×ª ×”× ×“×¨×©×•×ª
            for col in required_columns:
                if col in df.columns and df[col].isna().any():
                    null_count = df[col].isna().sum()
                    msg = f"Column '{col}' has {null_count} missing values"
                    logger.error(f"âœ— {msg}")
                    return False, msg

            logger.info("   âœ“ No missing values in required columns")

        # ×”×¦×œ×—×”
        msg = f"Clean data validation passed: {len(df)} rows, all required columns present"
        logger.success(f"âœ“ {msg}")
        return True, msg

    except json.JSONDecodeError as e:
        msg = f"Invalid JSON in contract file: {str(e)}"
        logger.error(f"âœ— {msg}")
        return False, msg

    except pd.errors.EmptyDataError as e:
        msg = f"Clean data file is empty: {str(e)}"
        logger.error(f"âœ— {msg}")
        return False, msg

    except Exception as e:
        msg = f"Error validating clean data: {str(e)}"
        logger.error(f"âœ— {msg}")
        return False, msg


def validate_dataset_contract(contract_path: str) -> Tuple[bool, str]:
    """
    ×‘×“×™×§×ª ×ª×§×™× ×•×ª ×—×•×–×” ×”× ×ª×•× ×™×.
    ××•×•×“× ×©×”×§×•×‘×¥ ×”×•× JSON ×ª×§×™×Ÿ ×•××›×™×œ ××ª ×›×œ ×”×©×“×•×ª ×”× ×“×¨×©×™×.

    Args:
        contract_path: × ×ª×™×‘ ×œ×§×•×‘×¥ ×”×—×•×–×” (dataset_contract.json)

    Returns:
        (True, ×”×•×“×¢×”) ×× ×”×—×•×–×” ×ª×§×™×Ÿ
        (False, ×”×•×“×¢×”) ×× ×™×© ×‘×¢×™×” ×‘×—×•×–×”
    """
    logger.info(f"ğŸ” Validating dataset contract: {contract_path}")

    try:
        # ×‘×“×™×§×ª ×§×™×•× ×”×§×•×‘×¥
        contract_path_obj = Path(contract_path)
        if not contract_path_obj.exists():
            msg = f"Contract file not found: {contract_path}"
            logger.error(f"âœ— {msg}")
            return False, msg

        # ×˜×¢×™× ×ª ×”-JSON
        logger.info("   Loading contract JSON...")
        with open(contract_path, "r", encoding="utf-8") as f:
            contract = json.load(f)

        # ×©×“×•×ª ×—×•×‘×” ×©×¦×¨×™×›×™× ×œ×”×™×•×ª ×‘×—×•×–×”
        required_fields = ["schema", "required_columns", "constraints"]

        # ×‘×“×™×§×ª ×©×“×•×ª ×—×•×‘×”
        missing_fields = []
        for field in required_fields:
            if field not in contract:
                missing_fields.append(field)

        if missing_fields:
            # ×‘×“×™×§×” ×× ×™×© ×œ×¤×—×•×ª schema
            if "schema" not in contract:
                msg = f"Contract missing critical field: schema"
                logger.error(f"âœ— {msg}")
                return False, msg
            else:
                logger.warning(f"âš  Contract missing optional fields: {missing_fields}")

        # ×‘×“×™×§×ª required_columns ×× ×§×™×™×
        if "required_columns" in contract:
            required_columns = contract["required_columns"]

            # ×‘×“×™×§×” ×©×–×” list
            if not isinstance(required_columns, list):
                msg = "required_columns must be a list"
                logger.error(f"âœ— {msg}")
                return False, msg

            # ×‘×“×™×§×” ×©×œ× ×¨×™×§
            if len(required_columns) == 0:
                msg = "required_columns cannot be empty"
                logger.error(f"âœ— {msg}")
                return False, msg

            logger.info(f"   âœ“ required_columns: {len(required_columns)} columns defined")

        # ×‘×“×™×§×ª schema ×× ×§×™×™×
        if "schema" in contract:
            schema = contract["schema"]

            # ×‘×“×™×§×” ×©×–×” dict
            if not isinstance(schema, dict):
                msg = "schema must be a dictionary"
                logger.error(f"âœ— {msg}")
                return False, msg

            # ×‘×“×™×§×” ×©×œ× ×¨×™×§
            if len(schema) == 0:
                msg = "schema cannot be empty"
                logger.error(f"âœ— {msg}")
                return False, msg

            logger.info(f"   âœ“ schema: {len(schema)} fields defined")

        # ×¡×¤×™×¨×ª ×¢××•×“×•×ª × ×“×¨×©×•×ª ×œ×”×•×“×¢×ª ×”×”×¦×œ×—×”
        num_columns = 0
        if "required_columns" in contract:
            num_columns = len(contract["required_columns"])
        elif "schema" in contract and "columns" in contract["schema"]:
            num_columns = len(contract["schema"]["columns"])

        # ×”×¦×œ×—×”
        msg = f"Dataset contract validation passed: {num_columns} required columns defined"
        logger.success(f"âœ“ {msg}")
        return True, msg

    except json.JSONDecodeError as e:
        msg = f"Invalid JSON format: {str(e)}"
        logger.error(f"âœ— {msg}")
        return False, msg

    except FileNotFoundError as e:
        msg = f"Contract file not found: {str(e)}"
        logger.error(f"âœ— {msg}")
        return False, msg

    except Exception as e:
        msg = f"Error validating contract: {str(e)}"
        logger.error(f"âœ— {msg}")
        return False, msg


def validate_features(features_path: str, contract_path: str = None) -> Tuple[bool, str]:
    """
    ×‘×“×™×§×ª ×ª×§×™× ×•×ª ×§×•×‘×¥ ×”-features.
    ××•×•×“× ×©×”×§×•×‘×¥ ×§×™×™× ×•××›×™×œ × ×ª×•× ×™×.

    Args:
        features_path: × ×ª×™×‘ ×œ×§×•×‘×¥ ×”-features (features.csv)
        contract_path: × ×ª×™×‘ ×œ×§×•×‘×¥ ×”×—×•×–×” (××•×¤×¦×™×•× ×œ×™, ×œ×‘×“×™×§×•×ª × ×•×¡×¤×•×ª)

    Returns:
        (True, ×”×•×“×¢×”) ×× ×”-features ×ª×§×™× ×™×
        (False, ×”×•×“×¢×”) ×× ×™×© ×‘×¢×™×”
    """
    logger.info(f"ğŸ” Validating features: {features_path}")

    try:
        # ×‘×“×™×§×ª ×§×™×•× ×§×•×‘×¥ ×”-features
        features_path_obj = Path(features_path)
        if not features_path_obj.exists():
            msg = f"Features file not found: {features_path}"
            logger.error(f"âœ— {msg}")
            return False, msg

        # ×˜×¢×™× ×ª ×”×§×•×‘×¥
        logger.info("   Loading features file...")
        df = pd.read_csv(features_path)

        # ×‘×“×™×§×” ×©×™×© ×©×•×¨×•×ª
        if len(df) == 0:
            msg = "Features file is empty (no rows)"
            logger.error(f"âœ— {msg}")
            return False, msg

        # ×‘×“×™×§×” ×©×™×© ×¢××•×“×•×ª (features)
        if len(df.columns) == 0:
            msg = "Features file has no columns"
            logger.error(f"âœ— {msg}")
            return False, msg

        logger.info(f"   âœ“ Features loaded: {len(df)} rows, {len(df.columns)} features")

        # ×‘×“×™×§×•×ª ××•×¤×¦×™×•× ×œ×™×•×ª ××•×œ ×”×—×•×–×”
        if contract_path:
            contract_path_obj = Path(contract_path)
            if contract_path_obj.exists():
                logger.info("   Checking against contract...")
                with open(contract_path, "r", encoding="utf-8") as f:
                    contract = json.load(f)

                # ×‘×“×™×§×” ×× ×™×© ××™× ×™××•× features ××•×’×“×¨
                if "min_features" in contract:
                    min_features = contract["min_features"]
                    if len(df.columns) < min_features:
                        msg = f"Not enough features: {len(df.columns)} < {min_features} required"
                        logger.error(f"âœ— {msg}")
                        return False, msg
                    logger.info(f"   âœ“ Minimum features requirement met")

        # ×”×¦×œ×—×”
        msg = f"Features validation passed: {len(df)} rows, {len(df.columns)} features"
        logger.success(f"âœ“ {msg}")
        return True, msg

    except pd.errors.EmptyDataError as e:
        msg = f"Features file is empty or corrupted: {str(e)}"
        logger.error(f"âœ— {msg}")
        return False, msg

    except pd.errors.ParserError as e:
        msg = f"Failed to parse features CSV: {str(e)}"
        logger.error(f"âœ— {msg}")
        return False, msg

    except json.JSONDecodeError as e:
        msg = f"Invalid JSON in contract file: {str(e)}"
        logger.error(f"âœ— {msg}")
        return False, msg

    except Exception as e:
        msg = f"Error validating features: {str(e)}"
        logger.error(f"âœ— {msg}")
        return False, msg


def validate_model_outputs(
    model_path: str,
    eval_path: str,
    card_path: str
) -> Tuple[bool, str]:
    """
    ×‘×“×™×§×ª ×ª×§×™× ×•×ª ×›×œ ×ª×•×¦×¨×™ ×”××•×“×œ.
    ××•×•×“× ×©×”××•×“×œ, ×“×•×— ×”×”×¢×¨×›×”, ×•×›×¨×˜×™×¡ ×”××•×“×œ × ×•×¦×¨×• ×•×ª×§×™× ×™×.

    Args:
        model_path: × ×ª×™×‘ ×œ×§×•×‘×¥ ×”××•×“×œ (model.pkl)
        eval_path: × ×ª×™×‘ ×œ×“×•×— ×”×”×¢×¨×›×” (evaluation_report.md)
        card_path: × ×ª×™×‘ ×œ×›×¨×˜×™×¡ ×”××•×“×œ (model_card.md)

    Returns:
        (True, ×”×•×“×¢×”) ×× ×›×œ ×”×ª×•×¦×¨×™× ×ª×§×™× ×™×
        (False, ×”×•×“×¢×”) ×× ×—×¡×¨ ×§×•×‘×¥ ××• ×™×© ×‘×¢×™×”
    """
    logger.info("ğŸ” Validating model outputs")
    logger.info(f"   Model: {model_path}")
    logger.info(f"   Evaluation: {eval_path}")
    logger.info(f"   Model Card: {card_path}")

    try:
        # ×‘×“×™×§×” 1: ×§×™×•× ×§×•×‘×¥ ×”××•×“×œ
        model_path_obj = Path(model_path)
        if not model_path_obj.exists():
            msg = f"Model file not found: {model_path}"
            logger.error(f"âœ— {msg}")
            return False, msg

        # ×‘×“×™×§×” ×©×”×§×•×‘×¥ ×œ× ×¨×™×§
        if model_path_obj.stat().st_size == 0:
            msg = f"Model file is empty: {model_path}"
            logger.error(f"âœ— {msg}")
            return False, msg

        logger.info("   âœ“ Model file exists and not empty")

        # ×‘×“×™×§×” 2: ×§×™×•× ×“×•×— ×”×”×¢×¨×›×”
        eval_path_obj = Path(eval_path)
        if not eval_path_obj.exists():
            msg = f"Evaluation report not found: {eval_path}"
            logger.error(f"âœ— {msg}")
            return False, msg

        # ×§×¨×™××ª ×“×•×— ×”×”×¢×¨×›×” ×•×‘×“×™×§×” ×©×™×© ×ª×•×›×Ÿ
        with open(eval_path, "r", encoding="utf-8") as f:
            eval_content = f.read()

        if len(eval_content.strip()) == 0:
            msg = "Evaluation report is empty"
            logger.error(f"âœ— {msg}")
            return False, msg

        logger.info("   âœ“ Evaluation report exists and has content")

        # ×‘×“×™×§×” 3: ×§×™×•× ×›×¨×˜×™×¡ ×”××•×“×œ
        card_path_obj = Path(card_path)
        if not card_path_obj.exists():
            msg = f"Model card not found: {card_path}"
            logger.error(f"âœ— {msg}")
            return False, msg

        # ×§×¨×™××ª ×›×¨×˜×™×¡ ×”××•×“×œ
        with open(card_path, "r", encoding="utf-8") as f:
            card_content = f.read()

        if len(card_content.strip()) == 0:
            msg = "Model card is empty"
            logger.error(f"âœ— {msg}")
            return False, msg

        logger.info("   âœ“ Model card exists and has content")

        # ×‘×“×™×§×” 4: ×‘×“×™×§×ª ×¡×§×©× ×™× × ×“×¨×©×™× ×‘×›×¨×˜×™×¡ ×”××•×“×œ
        required_sections = ["Purpose", "Data", "Metrics", "Limitations", "Ethical"]
        missing_sections = []

        for section in required_sections:
            # ×‘×“×™×§×” case-insensitive
            if section.lower() not in card_content.lower():
                missing_sections.append(section)

        if missing_sections:
            msg = f"Model card missing section: {missing_sections[0]}"
            logger.error(f"âœ— {msg}")
            return False, msg

        logger.info("   âœ“ Model card contains all required sections")

        # ×”×¦×œ×—×” - ×›×œ ×”×§×‘×¦×™× ×§×™×™××™× ×•×ª×§×™× ×™×
        msg = "Model outputs validation passed: all files present and valid"
        logger.success(f"âœ“ {msg}")
        return True, msg

    except FileNotFoundError as e:
        msg = f"File not found: {str(e)}"
        logger.error(f"âœ— {msg}")
        return False, msg

    except PermissionError as e:
        msg = f"Permission denied reading file: {str(e)}"
        logger.error(f"âœ— {msg}")
        return False, msg

    except Exception as e:
        msg = f"Error validating model outputs: {str(e)}"
        logger.error(f"âœ— {msg}")
        return False, msg


# =============================================================================
# ××—×œ×§×ª DataValidator ×œ×ª××™××•×ª ××—×•×¨×”
# =============================================================================

class DataValidator:
    """
    ××—×œ×§×” ×¢×•×˜×¤×ª ×œ×¤×•× ×§×¦×™×•×ª ×”-validation.
    ×©×•××¨×ª ×¢×œ ×ª××™××•×ª ××—×•×¨×” ×¢× ×”×§×•×“ ×”×§×™×™×.
    """

    def __init__(self):
        """××ª×—×•×œ ×”Validator"""
        self.logger = logger.bind(name="DataValidator")

    def validate_raw_data(self, file_path) -> Tuple[bool, str]:
        """×¢×•×˜×£ ××ª ×”×¤×•× ×§×¦×™×” validate_raw_data"""
        return validate_raw_data(str(file_path))

    def validate_clean_data(self, file_path, contract_path=None) -> Tuple[bool, str]:
        """×¢×•×˜×£ ××ª ×”×¤×•× ×§×¦×™×” validate_clean_data"""
        if contract_path:
            return validate_clean_data(str(file_path), str(contract_path))
        # ×× ××™×Ÿ contract, ×¤×©×•×˜ ×‘×“×•×§ ×©×”×§×•×‘×¥ ×§×™×™× ×•×ª×§×™×Ÿ
        return validate_raw_data(str(file_path))

    def validate_dataset_contract(self, contract_path) -> Tuple[bool, str]:
        """×¢×•×˜×£ ××ª ×”×¤×•× ×§×¦×™×” validate_dataset_contract"""
        return validate_dataset_contract(str(contract_path))

    def validate_features(self, features_path, contract_path=None) -> Tuple[bool, str]:
        """×¢×•×˜×£ ××ª ×”×¤×•× ×§×¦×™×” validate_features"""
        return validate_features(str(features_path), str(contract_path) if contract_path else None)

    def validate_model_outputs(self, model_path, eval_path, card_path) -> Tuple[bool, str]:
        """×¢×•×˜×£ ××ª ×”×¤×•× ×§×¦×™×” validate_model_outputs"""
        return validate_model_outputs(str(model_path), str(eval_path), str(card_path))


# =============================================================================
# ×‘×“×™×§×ª × ×ª×•× ×™× ××•×œ constraints
# =============================================================================

def validate_against_constraints(df: pd.DataFrame, contract: dict) -> Tuple[bool, str]:
    """
    ×‘×“×™×§×ª DataFrame ××•×œ ××™×œ×•×¦×™ ×”×—×•×–×”.
    ××•×•×“× ×©×›×œ ×”× ×ª×•× ×™× ×¢×•××“×™× ×‘-constraints ×©×”×•×’×“×¨×•.

    Args:
        df: ×”× ×ª×•× ×™× ×œ×‘×“×™×§×”
        contract: ×”×—×•×–×” ×¢× ×”-constraints

    Returns:
        (True, ×”×•×“×¢×”) ×× ×¢×•×‘×¨
        (False, ×¨×©×™××ª ×©×’×™××•×ª) ×× × ×›×©×œ

    Example:
        >>> df = pd.read_csv("clean_data.csv")
        >>> contract = json.load(open("dataset_contract.json"))
        >>> is_valid, msg = validate_against_constraints(df, contract)
    """
    logger.info("ğŸ” Validating data against contract constraints")

    errors = []
    constraints = contract.get('constraints', {})

    if not constraints:
        logger.warning("âš  No constraints defined in contract")
        return True, "No constraints to validate"

    for column, rules in constraints.items():
        # ×‘×“×™×§×ª ×§×™×•× ×¢××•×“×”
        if column not in df.columns:
            if rules.get('required', False):
                errors.append(f"×¢××•×“×” ×—×•×‘×” ×—×¡×¨×”: {column}")
            continue

        col_data = df[column]

        # ×‘×“×™×§×ª ×¢×¨×›×™× ××¡×¤×¨×™×™×
        if rules.get('type') == 'numeric':
            try:
                # × ×™×¡×™×•×Ÿ ×œ×”××™×¨ ×œ××¡×¤×¨×™× (××˜×¤×œ ×‘×¤×•×¨××˜ ××—×™×¨×™× ×¢× â‚¹)
                numeric_data = pd.to_numeric(
                    col_data.astype(str).str.replace('[â‚¹,]', '', regex=True),
                    errors='coerce'
                )

                # ×‘×“×™×§×ª ××™× ×™××•×
                if 'min' in rules:
                    below_min = numeric_data < rules['min']
                    if below_min.any():
                        count = below_min.sum()
                        errors.append(f"{column}: {count} ×¢×¨×›×™× ××ª×—×ª ×œ××™× ×™××•× {rules['min']}")

                # ×‘×“×™×§×ª ××§×¡×™××•×
                if 'max' in rules:
                    above_max = numeric_data > rules['max']
                    if above_max.any():
                        count = above_max.sum()
                        errors.append(f"{column}: {count} ×¢×¨×›×™× ××¢×œ ××§×¡×™××•× {rules['max']}")

            except Exception as e:
                errors.append(f"{column}: ×©×’×™××” ×‘×”××¨×” ×œ××¡×¤×¨ - {str(e)}")

        # ×‘×“×™×§×ª ×™×™×—×•×“×™×•×ª
        if rules.get('unique', False):
            duplicates = col_data.duplicated().sum()
            if duplicates > 0:
                errors.append(f"{column}: {duplicates} ×¢×¨×›×™× ×›×¤×•×œ×™× (×¦×¨×™×š ×œ×”×™×•×ª ×™×™×—×•×“×™)")

        # ×‘×“×™×§×ª ×¢×¨×›×™× ×—×¡×¨×™×
        if rules.get('required', False):
            null_count = col_data.isnull().sum()
            if null_count > 0:
                errors.append(f"{column}: {null_count} ×¢×¨×›×™× ×—×¡×¨×™× (×©×“×” ×—×•×‘×”)")

    # ×¡×™×›×•×
    if errors:
        error_msg = "; ".join(errors)
        logger.error(f"âœ— Validation failed: {error_msg}")
        return False, error_msg

    validated_count = len(constraints)
    logger.success(f"âœ“ All {validated_count} constraints validated successfully")
    return True, f"Validated {validated_count} constraints successfully"


# =============================================================================
# ×¤×•× ×§×¦×™×™×ª ×¢×–×¨ ×œ×”×¨×¦×ª ×›×œ ×”validations
# =============================================================================

def validate_all(
    raw_data_path: str = None,
    clean_data_path: str = None,
    contract_path: str = None,
    features_path: str = None,
    model_path: str = None,
    eval_path: str = None,
    card_path: str = None
) -> Tuple[bool, dict]:
    """
    ×”×¨×¦×ª ×›×œ ×”validations ×‘×‘×ª ××—×ª.
    ×©×™××•×©×™ ×œ×‘×“×™×§×” ××”×™×¨×” ×©×œ ×›×œ ×”-Pipeline.

    Args:
        raw_data_path: × ×ª×™×‘ ×œ× ×ª×•× ×™× ×’×•×œ××™×™×
        clean_data_path: × ×ª×™×‘ ×œ× ×ª×•× ×™× ×× ×•×§×™×
        contract_path: × ×ª×™×‘ ×œ×—×•×–×”
        features_path: × ×ª×™×‘ ×œ-features
        model_path: × ×ª×™×‘ ×œ××•×“×œ
        eval_path: × ×ª×™×‘ ×œ×“×•×— ×”×¢×¨×›×”
        card_path: × ×ª×™×‘ ×œ×›×¨×˜×™×¡ ××•×“×œ

    Returns:
        (True, results) ×× ×›×œ ×”×‘×“×™×§×•×ª ×¢×‘×¨×•
        (False, results) ×× ×™×© ×›×©×œ×•×Ÿ
    """
    logger.info("=" * 50)
    logger.info("ğŸ” Running all validations")
    logger.info("=" * 50)

    results = {
        "raw_data": None,
        "clean_data": None,
        "contract": None,
        "features": None,
        "model_outputs": None
    }

    all_passed = True

    # ×‘×“×™×§×ª × ×ª×•× ×™× ×’×•×œ××™×™×
    if raw_data_path:
        is_valid, msg = validate_raw_data(raw_data_path)
        results["raw_data"] = {"valid": is_valid, "message": msg}
        if not is_valid:
            all_passed = False

    # ×‘×“×™×§×ª ×—×•×–×”
    if contract_path:
        is_valid, msg = validate_dataset_contract(contract_path)
        results["contract"] = {"valid": is_valid, "message": msg}
        if not is_valid:
            all_passed = False

    # ×‘×“×™×§×ª × ×ª×•× ×™× ×× ×•×§×™×
    if clean_data_path and contract_path:
        is_valid, msg = validate_clean_data(clean_data_path, contract_path)
        results["clean_data"] = {"valid": is_valid, "message": msg}
        if not is_valid:
            all_passed = False

    # ×‘×“×™×§×ª features
    if features_path:
        is_valid, msg = validate_features(features_path, contract_path)
        results["features"] = {"valid": is_valid, "message": msg}
        if not is_valid:
            all_passed = False

    # ×‘×“×™×§×ª ×ª×•×¦×¨×™ ××•×“×œ
    if model_path and eval_path and card_path:
        is_valid, msg = validate_model_outputs(model_path, eval_path, card_path)
        results["model_outputs"] = {"valid": is_valid, "message": msg}
        if not is_valid:
            all_passed = False

    # ×¡×™×›×•×
    logger.info("=" * 50)
    if all_passed:
        logger.success("âœ“ All validations passed!")
    else:
        logger.error("âœ— Some validations failed")
    logger.info("=" * 50)

    return all_passed, results


# =============================================================================
# ×‘×“×™×§×” ×¢×¦××™×ª
# =============================================================================

if __name__ == "__main__":
    # ×‘×“×™×§×” ××”×™×¨×” ×©×œ ×”×¤×•× ×§×¦×™×•×ª
    project_root = Path(__file__).parent.parent.parent

    print("\n" + "=" * 60)
    print("Testing Validators Module")
    print("=" * 60 + "\n")

    # ×‘×“×™×§×ª × ×ª×•× ×™× ×’×•×œ××™×™×
    raw_path = project_root / "data" / "raw" / "amazon_sales.csv"
    if raw_path.exists():
        is_valid, msg = validate_raw_data(str(raw_path))
        status = "PASS" if is_valid else "FAIL"
        print(f"\nRaw data: [{status}] {msg}")

    # ×‘×“×™×§×ª ×—×•×–×”
    contract_path = project_root / "data" / "contracts" / "dataset_contract.json"
    if contract_path.exists():
        is_valid, msg = validate_dataset_contract(str(contract_path))
        status = "PASS" if is_valid else "FAIL"
        print(f"\nContract: [{status}] {msg}")

    # ×‘×“×™×§×ª × ×ª×•× ×™× ×× ×•×§×™×
    clean_path = project_root / "data" / "processed" / "clean_data.csv"
    if clean_path.exists() and contract_path.exists():
        is_valid, msg = validate_clean_data(str(clean_path), str(contract_path))
        status = "PASS" if is_valid else "FAIL"
        print(f"\nClean data: [{status}] {msg}")

    print("\n" + "=" * 60)
    print("Validators test complete")
    print("=" * 60 + "\n")
